# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' @title Pairwise Partition Similarity Scores
#'
#' @description
#' Let \code{x} and \code{y} represent two partitions of a set of \eqn{n}
#' elements into, respectively, \eqn{K} and \eqn{L}
#' nonempty and pairwise disjoint subsets.
#' For instance, these can be two clusterings of a dataset with
#' \eqn{n} observations specified by two vectors of labels.
#' The functions described in this section quantify the similarity between
#' \code{x} and \code{y}. They can be used as external cluster
#' validity measures, i.e., in the presence of reference (ground-truth)
#' partitions.
#'
#' @details
#' Every index except \code{mi_score()} (which computes the mutual
#' information score) outputs 1 given two identical partitions.
#' Note that partitions are always defined up to a bijection of the set of
#' possible labels, e.g., (1, 1, 2, 1) and (4, 4, 2, 4)
#' represent the same 2-partition.
#'
#' \code{rand_score()} gives the Rand score (the "probability" of agreement
#' between the two partitions) and
#' \code{adjusted_rand_score()} is its version corrected for chance,
#' see (Hubert, Arabie, 1985),
#' its expected value is 0.0 given two independent partitions.
#' Due to the adjustment, the resulting index might also be negative
#' for some inputs.
#'
#' Similarly, \code{fm_score()} gives the Fowlkes-Mallows (FM) score
#' and \code{adjusted_fm_score()} is its adjusted-for-chance version,
#' see (Hubert, Arabie, 1985).
#'
#' Note that both the (unadjusted) Rand and FM scores are bounded from below
#' by \eqn{1/(K+1)} if \eqn{K=L}, hence their adjusted versions are preferred.
#'
#' \code{mi_score()}, \code{adjusted_mi_score()} and
#' \code{normalized_mi_score()} are information-theoretic
#' scores, based on mutual information,
#' see the definition of \eqn{AMI_{sum}} and \eqn{NMI_{sum}}
#' in (Vinh et al., 2010).
#'
#' \code{normalized_accuracy()} is defined as
#' \eqn{(Accuracy(C_\sigma)-1/L)/(1-1/L)}, where \eqn{C_\sigma} is a version
#' of the confusion matrix for given \code{x} and \code{y},
#' \eqn{K \leq L}, with columns permuted based on the solution to the
#' Maximal Linear Sum Assignment Problem.
#' \eqn{Accuracy(C_\sigma)} is sometimes referred to as Purity,
#' e.g., in (Rendon et al. 2011).
#'
#' \code{pair_sets_index()} gives the  Pair Sets Index (PSI)
#' adjusted for chance (Rezaei, Franti, 2016), \eqn{K \leq L}.
#' Pairing is based on the solution to the Linear Sum Assignment Problem
#' of a transformed version of the confusion matrix.
#'
#' @references
#' Hubert L., Arabie P., Comparing Partitions,
#' Journal of Classification 2(1), 1985, 193-218, esp. Eqs. (2) and (4).
#'
#' Rendon E., Abundez I., Arizmendi A., Quiroz E.M.,
#' Internal versus external cluster validation indexes,
#' International Journal of Computers and Communications 5(1), 2011, 27-34.
#'
#' Rezaei M., Franti P., Set matching measures for external cluster validity,
#' IEEE Transactions on Knowledge and Data Mining 28(8), 2016, 2173-2186.
#'
#' Vinh N.X., Epps J., Bailey J.,
#' Information theoretic measures for clusterings comparison:
#' Variants, properties, normalization and correction for chance,
#' Journal of Machine Learning Research 11, 2010, 2837-2854.
#'
#'
#' @param x an integer vector of length n (or an object coercible to)
#' representing a K-partition of an n-set,
#' or a confusion matrix with K rows and L columns (see \code{table(x, y)})
#'
#' @param y an integer vector of length n (or an object coercible to)
#' representing an L-partition of the same set),
#' or NULL (if x is an K*L confusion matrix)
#'
#' @return A single real value giving the similarity score.
#'
#' @examples
#' y_true <- iris[[5]]
#' y_pred <- kmeans(as.matrix(iris[1:4]), 3)$cluster
#' adjusted_rand_score(y_true, y_pred)
#' rand_score(table(y_true, y_pred)) # the same
#' adjusted_fm_score(y_true, y_pred)
#' fm_score(y_true, y_pred)
#' mi_score(y_true, y_pred)
#' normalized_mi_score(y_true, y_pred)
#' adjusted_mi_score(y_true, y_pred)
#' normalized_accuracy(y_true, y_pred)
#' pair_sets_index(y_true, y_pred)
#'
#' @rdname comparing_partitions
#' @export
adjusted_rand_score <- function(x, y = NULL) {
    .Call(`_genieclust_adjusted_rand_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
rand_score <- function(x, y = NULL) {
    .Call(`_genieclust_rand_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
adjusted_fm_score <- function(x, y = NULL) {
    .Call(`_genieclust_adjusted_fm_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
fm_score <- function(x, y = NULL) {
    .Call(`_genieclust_fm_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
mi_score <- function(x, y = NULL) {
    .Call(`_genieclust_mi_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
normalized_mi_score <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_mi_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
adjusted_mi_score <- function(x, y = NULL) {
    .Call(`_genieclust_adjusted_mi_score`, x, y)
}

#' @rdname comparing_partitions
#' @export
normalized_accuracy <- function(x, y = NULL) {
    .Call(`_genieclust_normalized_accuracy`, x, y)
}

#' @rdname comparing_partitions
#' @export
pair_sets_index <- function(x, y = NULL) {
    .Call(`_genieclust_pair_sets_index`, x, y)
}

#' @title Internal Cluster Validity Measures
#'
#' @description
#' Implements a number of cluster validity indices critically
#' reviewed in (Gagolewski, Bartoszuk, Cena, 2021). See Section 2
#' therein for the respective definitions.
#'
#' The greater the index value, the more \emph{valid} (whatever that means)
#' the assessed partition. For consistency, the Ball-Hall and
#' Davies-Bouldin indexes take negative values.
#'
#'
#' @param X numeric matrix with \code{n} rows and \code{d} columns,
#'     representing \code{n} points in a \code{d}-dimensional space
#'
#' @param y vector of \code{n} integer labels with elements in \eqn{[1, K]},
#'     representing a partition whose \emph{quality} is to be
#'     assessed; \code{y[i]} is the cluster ID of the \code{i}-th point,
#'     \code{X[i, ]}
#'
#' @param K number of clusters, equal to \code{max(y)}
#'
#' @param M number of nearest neighbours
#'
#' @param lowercase_delta an integer between 1 and 6, denoting
#'     \eqn{d_1}, ..., \eqn{d_6} in the definition
#'     of the generalised Dunn index (numerator)
#'
#' @param uppercase_delta an integer between 1 and 3, denoting
#'     \eqn{D_1}, ..., \eqn{D_3} in the definition
#'     of the generalised Dunn index (denominator)
#'
#' @param owa_numerator,owa_denominator single string defining
#'     the OWA operator to use in the definition of the DuNN index;
#'     one of: \code{"Mean"}, \code{"Min"}, \code{"Max"}, \code{"Const"},
#'     \code{"SMin:M"}, \code{"SMax:M"}, where \code{M} is an integer
#'     defining the number of nearest neighbours.
#'
#' @return A single numeric value.
#'
#' @references
#' G.H. Ball, D.J. Hall,
#' ISODATA: A novel method of data analysis and pattern classification,
#' Technical report No. AD699616, Stanford Research Institute, 1965.
#'
#' J. Bezdek, N. Pal, Some new indexes of cluster validity,
#' IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 28
#' 1998, pp. 301--315, \doi{10.1109/3477.678624}.
#'
#' T. Calinski, J. Harabasz. A dendrite method for cluster analysis,
#' Communications in Statistics, 3(1), 1974, pp. 1-27,
#' \doi{10.1080/03610927408827101}.
#'
#' D.L. Davies, D.W. Bouldin,
#' A Cluster Separation Measure,
#' IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2),
#' 1979, pp. 224-227, \doi{10.1109/TPAMI.1979.4766909}.
#'
#' J.C. Dunn, A Fuzzy Relative of the ISODATA Process and Its Use in Detecting
#' Compact Well-Separated Clusters, Journal of Cybernetics 3(3), 1973,
#' pp. 32-57, \doi{10.1080/01969727308546046}.
#'
#' M. Gagolewski, M. Bartoszuk, A. Cena,
#' Are cluster validity measures (in)valid?, Information Sciences 581,
#' 620â€“636, 2021, \doi{10.1016/j.ins.2021.10.004};
#' preprint: \url{https://raw.githubusercontent.com/gagolews/bibliography/master/preprints/2021cvi.pdf}.
#'
#' P.J. Rousseeuw, Silhouettes: A Graphical Aid to the Interpretation and
#' Validation of Cluster Analysis, Computational and Applied Mathematics 20,
#' 1987, pp. 53-65, \doi{10.1016/0377-0427(87)90125-7}.
#'
#'
#'
#' @examples
#' X <- as.matrix(iris[,1:4])
#' X[,] <- jitter(X)  # otherwise we get a non-unique solution
#' y <- as.integer(iris[[5]])
#' calinski_harabasz_index(X, y, max(y))
#' calinski_harabasz_index(X, sample(1:3, nrow(X), replace=TRUE), max(y))
#'
#' @name cluster_validity_measures
#' @rdname cluster_validity_measures
#' @export
calinski_harabasz_index <- function(X, y, K) {
    .Call(`_genieclust_calinski_harabasz_index`, X, y, K)
}

#' @rdname cluster_validity_measures
#' @export
dunnowa_index <- function(X, y, K, M = 10L, owa_numerator = "Min", owa_denominator = "Max") {
    .Call(`_genieclust_dunnowa_index`, X, y, K, M, owa_numerator, owa_denominator)
}

#' @rdname cluster_validity_measures
#' @export
generalised_dunn_index <- function(X, y, K, lowercase_delta, uppercase_delta) {
    .Call(`_genieclust_generalised_dunn_index`, X, y, K, lowercase_delta, uppercase_delta)
}

#' @rdname cluster_validity_measures
#' @export
negated_ball_hall_index <- function(X, y, K) {
    .Call(`_genieclust_negated_ball_hall_index`, X, y, K)
}

#' @rdname cluster_validity_measures
#' @export
negated_davies_bouldin_index <- function(X, y, K) {
    .Call(`_genieclust_negated_davies_bouldin_index`, X, y, K)
}

#' @rdname cluster_validity_measures
#' @export
silhouette_index <- function(X, y, K) {
    .Call(`_genieclust_silhouette_index`, X, y, K)
}

#' @rdname cluster_validity_measures
#' @export
silhouette_w_index <- function(X, y, K) {
    .Call(`_genieclust_silhouette_w_index`, X, y, K)
}

#' @rdname cluster_validity_measures
#' @export
wcnn_index <- function(X, y, K, M = 10L) {
    .Call(`_genieclust_wcnn_index`, X, y, K, M)
}

#' @rdname cluster_validity_measures
#' @export
wcss_index <- function(X, y, K) {
    .Call(`_genieclust_wcss_index`, X, y, K)
}

.mst.default <- function(X, distance = "euclidean", M = 1L, cast_float32 = TRUE, verbose = FALSE) {
    .Call(`_genieclust_dot_mst_default`, X, distance, M, cast_float32, verbose)
}

.mst.dist <- function(d, M = 1L, verbose = FALSE) {
    .Call(`_genieclust_dot_mst_dist`, d, M, verbose)
}

.genie <- function(mst, k, gini_threshold, postprocess, detect_noise, verbose) {
    .Call(`_genieclust_dot_genie`, mst, k, gini_threshold, postprocess, detect_noise, verbose)
}

.gclust <- function(mst, gini_threshold, verbose) {
    .Call(`_genieclust_dot_gclust`, mst, gini_threshold, verbose)
}

#' @title Inequity (Inequality) Measures
#'
#' @description
#' \code{gini_index()} gives the normalised Gini index,
#' \code{bonferroni_index()} implements the Bonferroni index, and
#' \code{devergottini_index()} implements the De Vergottini index.
#'
#' @details
#' These indices can be used to quantify the "inequity" of a numeric sample.
#' They can be perceived as measures of data dispersion.
#' For constant vectors (perfect equity), the indices yield values of 0.
#' Vectors with all elements but one equal to 0 (perfect inequity),
#' are assigned scores of 1.
#' They follow the Pigou-Dalton principle (are Schur-convex):
#' setting \eqn{x_i = x_i - h} and \eqn{x_j = x_j + h} with \eqn{h > 0}
#' and \eqn{x_i - h \geq  x_j + h} (taking from the "rich" and
#' giving to the "poor") decreases the inequity.
#'
#' These indices have applications in economics, amongst others.
#' The Genie clustering algorithm uses the Gini index as a measure
#' of the inequality of cluster sizes.
#'
#'
#' The normalised  Gini index is given by:
#' \deqn{
#'     G(x_1,\dots,x_n) = \frac{
#'     \sum_{i=1}^{n} (n-2i+1) x_{\sigma(n-i+1)}
#'     }{
#'     (n-1) \sum_{i=1}^n x_i
#'     },
#' }
#'
#' The normalised Bonferroni index is given by:
#' \deqn{
#'     B(x_1,\dots,x_n) = \frac{
#'     \sum_{i=1}^{n}  (n-\sum_{j=1}^i \frac{n}{n-j+1})
#'          x_{\sigma(n-i+1)}
#'     }{
#'     (n-1) \sum_{i=1}^n x_i
#'     }.
#' }
#'
#' The normalised De Vergottini index is given by:
#' \deqn{
#'     V(x_1,\dots,x_n) =
#'     \frac{1}{\sum_{i=2}^n \frac{1}{i}} \left(
#'        \frac{ \sum_{i=1}^n \left( \sum_{j=i}^{n} \frac{1}{j}\right)
#'        x_{\sigma(n-i+1)} }{\sum_{i=1}^{n} x_i} - 1
#'     \right).
#' }
#'
#' Here, \eqn{\sigma} is an ordering permutation of \eqn{(x_1,\dots,x_n)}.
#'
#' Time complexity: \eqn{O(n)} for sorted (increasingly) data.
#' Otherwise, the vector will be sorted.
#'
#'
#' @references
#' Bonferroni C., Elementi di Statistica Generale, Libreria Seber,
#' Firenze, 1930.
#'
#' Gagolewski M., Bartoszuk M., Cena A., Genie: A new, fast, and
#' outlier-resistant hierarchical clustering algorithm,
#' Information Sciences 363, 2016, pp. 8-23. doi:10.1016/j.ins.2016.05.003
#'
#' Gini C., Variabilita e Mutabilita, Tipografia di Paolo Cuppini, Bologna, 1912.
#'
#'
#' @param x numeric vector of non-negative values
#'
#' @return The value of the inequity index, a number in \eqn{[0, 1]}.
#'
#' @examples
#' gini_index(c(2, 2, 2, 2, 2))  # no inequality
#' gini_index(c(0, 0, 10, 0, 0)) # one has it all
#' gini_index(c(7, 0, 3, 0, 0))  # give to the poor, take away from the rich
#' gini_index(c(6, 0, 3, 1, 0))  # (a.k.a. Pigou-Dalton principle)
#' bonferroni_index(c(2, 2, 2, 2, 2))
#' bonferroni_index(c(0, 0, 10, 0, 0))
#' bonferroni_index(c(7, 0, 3, 0, 0))
#' bonferroni_index(c(6, 0, 3, 1, 0))
#' devergottini_index(c(2, 2, 2, 2, 2))
#' devergottini_index(c(0, 0, 10, 0, 0))
#' devergottini_index(c(7, 0, 3, 0, 0))
#' devergottini_index(c(6, 0, 3, 1, 0))
#'
#' @rdname inequity
#' @export
gini_index <- function(x) {
    .Call(`_genieclust_gini_index`, x)
}

#' @rdname inequity
#' @export
bonferroni_index <- function(x) {
    .Call(`_genieclust_bonferroni_index`, x)
}

#' @rdname inequity
#' @export
devergottini_index <- function(x) {
    .Call(`_genieclust_devergottini_index`, x)
}

